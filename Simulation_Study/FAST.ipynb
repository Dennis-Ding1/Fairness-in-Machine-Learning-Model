{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fast.py\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) 读取 R 导出的 CSV\n",
    "# -----------------------------\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "val   = pd.read_csv(\"data/val.csv\")\n",
    "test  = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "FEATS = [\"age\", \"sbp\"]\n",
    "def to_tensor(df):\n",
    "  x = torch.tensor(df[FEATS].values, dtype=torch.float32, device=DEVICE)\n",
    "  a = torch.tensor(df[\"A\"].values.reshape(-1,1), dtype=torch.float32, device=DEVICE)\n",
    "  t = torch.tensor(df[\"Y\"].values, dtype=torch.float32, device=DEVICE)\n",
    "  e = torch.tensor(df[\"Delta\"].values, dtype=torch.float32, device=DEVICE)\n",
    "  return x, a, t, e\n",
    "\n",
    "x_tr, a_tr, t_tr, e_tr = to_tensor(train)\n",
    "x_va, a_va, t_va, e_va = to_tensor(val)\n",
    "x_te, a_te, t_te, e_te = to_tensor(test)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) 基线模型（DeepSurv 风格：MLP -> risk score）\n",
    "# -----------------------------\n",
    "class DeepSurv(nn.Module):\n",
    "  def __init__(self, in_dim):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "      nn.Linear(in_dim, 32), nn.ReLU(),\n",
    "      nn.Linear(32, 16), nn.ReLU(),\n",
    "      nn.Linear(16, 1)  # risk score (log hazard ratio)\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    return self.net(x)  # (N,1)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Cox 部分对数似然（全样本版，稳定）\n",
    "#    公式：sum_{i: e_i=1} (r_i - logsumexp(r_j, j in R_i))\n",
    "#    用时间降序 + 反向累积 log-sum-exp 近似实现\n",
    "# -----------------------------\n",
    "def cox_ph_loss(risk, time, event, eps=1e-8):\n",
    "  # risk: (N,1)\n",
    "  risk = risk.squeeze(-1)\n",
    "  # 排序：time 降序\n",
    "  order = torch.argsort(time, descending=True)\n",
    "  t_ord = time[order]; e_ord = event[order]; r_ord = risk[order]\n",
    "  # 反向累积 log-sum-exp 近似风险集\n",
    "  r_rev = torch.flip(r_ord, dims=[0])\n",
    "  cumsum_rev = torch.log(torch.cumsum(torch.exp(r_rev), dim=0) + eps)\n",
    "  log_riskset = torch.flip(cumsum_rev, dims=[0])\n",
    "  ll = torch.sum(e_ord * (r_ord - log_riskset))\n",
    "  return -ll  # minimize\n",
    "\n",
    "# -----------------------------\n",
    "# 4) MINE 互信息估计器（DV 下界 + moving average）\n",
    "#    输入是 (A, Z)，Z 可取 risk 表征（或隐藏层表示）\n",
    "# -----------------------------\n",
    "class MINE(nn.Module):\n",
    "  def __init__(self, in_dim=2, hidden=64):\n",
    "    super().__init__()\n",
    "    self.tnet = nn.Sequential(\n",
    "      nn.Linear(in_dim, hidden), nn.ReLU(),\n",
    "      nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "      nn.Linear(hidden, 1)\n",
    "    )\n",
    "    self.ma_et = None  # moving average of E[e^T] for stability\n",
    "\n",
    "  def forward(self, joint, marginal):\n",
    "    t_joint = self.tnet(joint)          # (N,1)\n",
    "    t_marg  = self.tnet(marginal)       # (N,1)\n",
    "    # DV lower bound: E[T] - log E[e^T]\n",
    "    et = torch.exp(t_marg)\n",
    "    mean_t = torch.mean(t_joint)\n",
    "    mean_et = torch.mean(et)\n",
    "\n",
    "    # moving-average trick to stabilize log-mean-exp term\n",
    "    ma_rate = 0.01\n",
    "    if self.ma_et is None:\n",
    "      self.ma_et = mean_et.detach()\n",
    "    else:\n",
    "      self.ma_et = (1 - ma_rate) * self.ma_et + ma_rate * mean_et.detach()\n",
    "\n",
    "    mi = mean_t - torch.log(self.ma_et + 1e-8)\n",
    "    return mi, mean_t.item(), mean_et.item()\n",
    "\n",
    "def sample_joint_and_marginal(a, z):\n",
    "  # a: (N,1), z: (N,1). joint 是配对 (a_i, z_i);\n",
    "  # marginal 是打乱 z 得到的 (a_i, z_pi)\n",
    "  idx = torch.randperm(a.shape[0], device=a.device)\n",
    "  joint = torch.cat([a, z], dim=1)\n",
    "  marg  = torch.cat([a, z[idx]], dim=1)\n",
    "  return joint, marg\n",
    "\n",
    "# -----------------------------\n",
    "# 5) 训练：Baseline & FAST\n",
    "# -----------------------------\n",
    "def evaluate_cindex(model, x, t, e):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    r = model(x).squeeze(-1).detach().cpu().numpy()\n",
    "  return concordance_index(t.cpu().numpy(), -r, e.cpu().numpy())  # -r: higher risk -> shorter time\n",
    "\n",
    "def group_cindex(model, x, t, e, a):\n",
    "  c0 = evaluate_cindex(model, x[a.squeeze()==0], t[a.squeeze()==0], e[a.squeeze()==0])\n",
    "  c1 = evaluate_cindex(model, x[a.squeeze()==1], t[a.squeeze()==1], e[a.squeeze()==1])\n",
    "  return c0, c1, abs(c0 - c1)\n",
    "\n",
    "# ---- Baseline ----\n",
    "baseline = DeepSurv(in_dim=len(FEATS)).to(DEVICE)\n",
    "opt_b = torch.optim.Adam(baseline.parameters(), lr=1e-3)\n",
    "EPOCHS = 200\n",
    "for ep in range(EPOCHS):\n",
    "  baseline.train()\n",
    "  opt_b.zero_grad()\n",
    "  risk = baseline(x_tr)\n",
    "  loss = cox_ph_loss(risk, t_tr, e_tr)\n",
    "  loss.backward(); opt_b.step()\n",
    "  if (ep+1) % 50 == 0:\n",
    "    c = evaluate_cindex(baseline, x_te, t_te, e_te)\n",
    "    print(f\"[Baseline] epoch {ep+1:03d} loss={loss.item():.3f} c-index={c:.3f}\")\n",
    "\n",
    "c_overall_base = evaluate_cindex(baseline, x_te, t_te, e_te)\n",
    "c0, c1, gap_base = group_cindex(baseline, x_te, t_te, e_te, a_te)\n",
    "print(f\"[Baseline] test c-index={c_overall_base:.3f} | group c0={c0:.3f} c1={c1:.3f} | ΔC={gap_base:.3f}\")\n",
    "\n",
    "# ---- FAST (with MINE) ----\n",
    "gamma = 0.5               # 公平强度（可尝试 0.1, 0.5, 1.0）\n",
    "mine_steps = 5            # 每轮先更新 MINE 的步数\n",
    "model = DeepSurv(in_dim=len(FEATS)).to(DEVICE)\n",
    "mine  = MINE(in_dim=2, hidden=64).to(DEVICE)\n",
    "opt_m = torch.optim.Adam(mine.parameters(),   lr=1e-3)\n",
    "opt_f = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 200\n",
    "for ep in range(EPOCHS):\n",
    "  # (a) 先更新 MINE：最大化 MI（因此对 MINE 做 gradient ascent -> 等价最小化(-MI)）\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    z = model(x_tr).detach()\n",
    "  for _ in range(mine_steps):\n",
    "    opt_m.zero_grad()\n",
    "    joint, marg = sample_joint_and_marginal(a_tr, z)\n",
    "    mi, _, _ = mine(joint, marg)\n",
    "    loss_m = -mi\n",
    "    loss_m.backward(); opt_m.step()\n",
    "\n",
    "  # (b) 再更新主模型：最小化 Cox + gamma * MI\n",
    "  model.train()\n",
    "  opt_f.zero_grad()\n",
    "  z = model(x_tr)\n",
    "  joint, marg = sample_joint_and_marginal(a_tr, z)\n",
    "  mi, _, _ = mine(joint.detach(), marg.detach())  # 固定 MINE，估计 MI；不回传到 MINE\n",
    "  loss_cox = cox_ph_loss(z, t_tr, e_tr)\n",
    "  loss_total = loss_cox + gamma * mi\n",
    "  loss_total.backward(); opt_f.step()\n",
    "\n",
    "  if (ep+1) % 50 == 0:\n",
    "    c = evaluate_cindex(model, x_te, t_te, e_te)\n",
    "    print(f\"[FAST] epoch {ep+1:03d} total={loss_total.item():.3f} c-index={c:.3f} mi={mi.item():.3f}\")\n",
    "\n",
    "c_overall_fast = evaluate_cindex(model, x_te, t_te, e_te)\n",
    "c0f, c1f, gap_fast = group_cindex(model, x_te, t_te, e_te, a_te)\n",
    "print(f\"[FAST γ={gamma}] test c-index={c_overall_fast:.3f} | group c0={c0f:.3f} c1={c1f:.3f} | ΔC={gap_fast:.3f}\")\n",
    "\n",
    "# 结果小结\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Baseline: c-index={c_overall_base:.3f}, ΔC={gap_base:.3f}\")\n",
    "print(f\"FAST(γ={gamma}): c-index={c_overall_fast:.3f}, ΔC={gap_fast:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
